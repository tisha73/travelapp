{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import os\n",
    "\n",
    "# # ✅ Load Excel File\n",
    "file_path = r\"C:\\Users\\RUTVIK\\Downloads\\OneWay_Final_Updated.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# # ✅ Keep Only 'Review' and 'Sentiment' Columns (Ensure correct case)\n",
    "df = df[['Review', 'Sentiment']]\n",
    "\n",
    "# # ✅ Establish MySQL Connection\n",
    "# DB_CONFIG = {\n",
    "#     \"host\": os.getenv(\"DB_HOST\"),  # Default to localhost if not set\n",
    "#     \"user\": os.getenv(\"DB_USER\"),\n",
    "#     \"password\": os.getenv(\"DB_PASSWORD\"),  # Replace with actual password\n",
    "#     \"database\": \"flight\"\n",
    "# }\n",
    "\n",
    "# conn = mysql.connector.connect(**DB_CONFIG)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # ✅ Create Table if Not Exists\n",
    "# cursor.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS Reviews (\n",
    "#         id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "#         review TEXT NOT NULL,\n",
    "#         sentiment ENUM('positive', 'negative', 'neutral') NOT NULL\n",
    "#     )\n",
    "# \"\"\")\n",
    "\n",
    "# # ✅ Insert Data into MySQL Table (Batch Insert for efficiency)\n",
    "# insert_query = \"INSERT INTO Reviews (review, sentiment) VALUES (%s, %s)\"\n",
    "# data = list(df.itertuples(index=False, name=None))  # Convert DataFrame to list of tuples\n",
    "\n",
    "# cursor.executemany(insert_query, data)  # Faster bulk insert\n",
    "\n",
    "# conn.commit()\n",
    "# cursor.close()\n",
    "# conn.close()\n",
    "\n",
    "# print(\"✅ Data imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Review', 'Sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sentiment\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "neutral     13634\n",
       "negative    13511\n",
       "positive    12834\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sentiment\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in c:\\users\\rutvik\\flight_pipeline\\flight\\lib\\site-packages (9.2.0)\n",
      "Requirement already satisfied: SQLAlchemy in c:\\users\\rutvik\\flight_pipeline\\flight\\lib\\site-packages (1.4.54)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rutvik\\flight_pipeline\\flight\\lib\\site-packages (from SQLAlchemy) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install mysql-connector-python SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DB_URL = os.getenv(\"DB_URL\")\n",
    "\n",
    "# Establish database connection using SQLAlchemy\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# Obtain raw connection to use cursor functionality\n",
    "connection = engine.raw_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Execute a query to fetch flight reviews using the cursor\n",
    "cursor.execute(\"SELECT * FROM Reviews\")\n",
    "reviews = cursor.fetchall()\n",
    "\n",
    "# Convert the fetched reviews into a pandas DataFrame\n",
    "columns = [desc[0] for desc in cursor.description]  # Get column names\n",
    "df = pd.DataFrame(reviews, columns=columns)\n",
    "\n",
    "# Text Cleaning Class\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.apply(self.clean_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "# Apply text cleaning\n",
    "cleaner = TextCleaner()\n",
    "df['cleaned_review'] = cleaner.transform(df['review'])\n",
    "\n",
    "# Encode sentiment labels\n",
    "sentiment_mapping = {\"positive\": 1, \"negative\": 0, \"neutral\": 2}\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map(sentiment_mapping)\n",
    "\n",
    "# Split dataset before transformation\n",
    "X_texts = df[\"cleaned_review\"].tolist()\n",
    "y_labels = df[\"sentiment\"].values\n",
    "\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
    "    X_texts, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=50000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_texts)\n",
    "X_test_tfidf = vectorizer.transform(X_test_texts)\n",
    "\n",
    "# Save vectorizer\n",
    "pickle.dump(vectorizer, open(\"tfidf_vectorizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest.pkl']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Random Forest Model\n",
    "# rf_model = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42)\n",
    "rf_model = RandomForestClassifier(n_estimators=1000, max_depth=200, min_samples_split=5, random_state=42, bootstrap=True)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "joblib.dump(rf_model, \"random_forest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train XGBoost Model\n",
    "# xgb_model = xgb.XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=6, eval_metric=\"mlogloss\")\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=10, subsample=0.8, reg_lambda=1, eval_metric=\"mlogloss\")\n",
    "\n",
    "xgb_model.fit(X_train_tfidf, y_train)\n",
    "joblib.dump(xgb_model, \"xgboost.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RUTVIK\\flight_pipeline\\flight\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4957 - loss: 0.9946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 77ms/step - accuracy: 0.4966 - loss: 0.9934 - val_accuracy: 1.0000 - val_loss: 0.0086\n",
      "Epoch 2/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9999 - loss: 0.0113"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 65ms/step - accuracy: 0.9999 - loss: 0.0113 - val_accuracy: 1.0000 - val_loss: 4.0707e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 1.1803e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 7.5808e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 7.5765e-04 - val_accuracy: 1.0000 - val_loss: 5.3497e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 3.8778e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 3.8757e-04 - val_accuracy: 1.0000 - val_loss: 2.7203e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 2.2750e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 2.2743e-04 - val_accuracy: 1.0000 - val_loss: 1.4236e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 1.8773e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 1.8770e-04 - val_accuracy: 1.0000 - val_loss: 9.4735e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 1.0884e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 1.0882e-04 - val_accuracy: 1.0000 - val_loss: 5.7826e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 7.9213e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 7.9179e-05 - val_accuracy: 1.0000 - val_loss: 3.3658e-06\n",
      "Epoch 10/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 5.9214e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 5.9191e-05 - val_accuracy: 1.0000 - val_loss: 2.0231e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 4.1164e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 4.1158e-05 - val_accuracy: 1.0000 - val_loss: 1.2907e-06\n",
      "Epoch 12/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 3.0218e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 3.0220e-05 - val_accuracy: 1.0000 - val_loss: 8.9071e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 2.8767e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 2.8759e-05 - val_accuracy: 1.0000 - val_loss: 6.8368e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 1.6020e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 1.6019e-05 - val_accuracy: 1.0000 - val_loss: 3.8628e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 1.4248e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 1.4250e-05 - val_accuracy: 1.0000 - val_loss: 3.8464e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 9.6512e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 9.6502e-06 - val_accuracy: 1.0000 - val_loss: 1.7343e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 8.1361e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 8.1344e-06 - val_accuracy: 1.0000 - val_loss: 1.0937e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 4.7343e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 4.7346e-06 - val_accuracy: 1.0000 - val_loss: 5.0048e-08\n",
      "Epoch 19/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 5.0447e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 5.0415e-06 - val_accuracy: 1.0000 - val_loss: 3.0384e-08\n",
      "Epoch 20/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 9.1756e-06 - val_accuracy: 1.0000 - val_loss: 6.3004e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Train Deep Learning Model\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_texts)\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train_texts), maxlen=100, padding=\"post\")\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test_texts), maxlen=100, padding=\"post\")\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=50000, output_dim=128, input_length=100),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adamax\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\"best_tf_sentiment_model.h5\", save_best_only=True)\n",
    "\n",
    "model.fit(X_train_seq, y_train, epochs=20, batch_size=128, validation_data=(X_test_seq, y_test), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "\n",
    "model.save(\"tf_sentiment_model.h5\")\n",
    "pickle.dump(tokenizer, open(\"tokenizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0000\n",
      "XGBoost Accuracy: 1.0000\n",
      "Deep Learning Model Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models\n",
    "rf_accuracy = accuracy_score(y_test, rf_model.predict(X_test_tfidf))\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_model.predict(X_test_tfidf))\n",
    "dl_accuracy = model.evaluate(X_test_seq, y_test, verbose=0)[1]\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"Deep Learning Model Accuracy: {dl_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Random Forest\n"
     ]
    }
   ],
   "source": [
    "# Select Best Model\n",
    "best_model_name = \"Random Forest\" if rf_accuracy >= xgb_accuracy and rf_accuracy >= dl_accuracy else (\n",
    "    \"XGBoost\" if xgb_accuracy >= dl_accuracy else \"Deep Learning\")\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "\n",
    "best_model = rf_model if best_model_name == \"Random Forest\" else (\n",
    "    xgb_model if best_model_name == \"XGBoost\" else model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sentiment Prediction Pipeline\n",
    "if best_model_name == \"Deep Learning\":\n",
    "    sentiment_pipeline = Pipeline([\n",
    "        ('text_cleaning', TextCleaner()),\n",
    "        ('tokenizer', tokenizer),\n",
    "        ('pad_sequences', None),  # Custom transformer for padding sequences can be added\n",
    "        ('classifier', best_model)\n",
    "    ])\n",
    "else:\n",
    "    sentiment_pipeline = Pipeline([\n",
    "        ('text_cleaning', TextCleaner()),\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', best_model)\n",
    "    ])\n",
    "\n",
    "joblib.dump(sentiment_pipeline, \"sentiment.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment  \\\n",
      "0      If you're looking for a great airline, this is...  positive   \n",
      "1      Loved every moment! The incredible was super W...  positive   \n",
      "2      Absolutely fantastic! The perfect was check-in...  positive   \n",
      "3      Avoid this airline! The horrible was aircraft ...  negative   \n",
      "4      Didn't impress, but didn't disappoint either. ...   neutral   \n",
      "...                                                  ...       ...   \n",
      "39974  A huge disappointment. The poor was security a...  negative   \n",
      "39975  Worst flight ever! The nightmarish was overhea...  negative   \n",
      "39976  A truly enjoyable flight! The comfortable was ...  positive   \n",
      "39977  Absolutely fantastic! The perfect was baggage ...  positive   \n",
      "39978  It was a disaster! The frustrating was bathroo...  negative   \n",
      "\n",
      "      Predicted_Sentiment  \n",
      "0                Positive  \n",
      "1                Positive  \n",
      "2                Positive  \n",
      "3                Negative  \n",
      "4                 Neutral  \n",
      "...                   ...  \n",
      "39974            Negative  \n",
      "39975            Negative  \n",
      "39976            Positive  \n",
      "39977            Positive  \n",
      "39978            Negative  \n",
      "\n",
      "[39979 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Prediction Function\n",
    "def predict_sentiment(review_text):\n",
    "    review_text_cleaned = cleaner.clean_text(review_text)\n",
    "    if best_model_name == \"Deep Learning\":\n",
    "        sequence = pad_sequences(tokenizer.texts_to_sequences([review_text_cleaned]), maxlen=100, padding=\"post\")\n",
    "        prediction = np.argmax(best_model.predict(sequence), axis=1)[0]\n",
    "    else:\n",
    "        prediction = best_model.predict(vectorizer.transform([review_text_cleaned]))[0]\n",
    "    sentiment_labels = {1: \"Positive\", 0: \"Negative\", 2: \"Neutral\"}\n",
    "    return sentiment_labels.get(prediction, \"Unknown\")\n",
    "\n",
    "# Obtain raw connection to use cursor functionality\n",
    "connection = engine.raw_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Execute a query to fetch flight reviews for inference\n",
    "cursor.execute(\"SELECT * FROM Reviews\")\n",
    "inference_reviews_data = cursor.fetchall()\n",
    "\n",
    "# Convert the fetched reviews into a pandas DataFrame\n",
    "columns = [desc[0] for desc in cursor.description]  # Get column names\n",
    "inference_reviews = pd.DataFrame(inference_reviews_data, columns=columns)\n",
    "\n",
    "# Apply the sentiment prediction function to the reviews\n",
    "inference_reviews[\"Predicted_Sentiment\"] = inference_reviews[\"review\"].apply(predict_sentiment)\n",
    "\n",
    "\n",
    "# Print the DataFrame with predicted sentiments\n",
    "print(inference_reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step\n",
      "Results for Random Forest:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-score: 1.0000\n",
      "Confusion Matrix:\n",
      " [[2702    0    0]\n",
      " [   0 2567    0]\n",
      " [   0    0 2727]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2702\n",
      "           1       1.00      1.00      1.00      2567\n",
      "           2       1.00      1.00      1.00      2727\n",
      "\n",
      "    accuracy                           1.00      7996\n",
      "   macro avg       1.00      1.00      1.00      7996\n",
      "weighted avg       1.00      1.00      1.00      7996\n",
      "\n",
      "==================================================\n",
      "Results for XGBoost:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-score: 1.0000\n",
      "Confusion Matrix:\n",
      " [[2702    0    0]\n",
      " [   0 2567    0]\n",
      " [   0    0 2727]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2702\n",
      "           1       1.00      1.00      1.00      2567\n",
      "           2       1.00      1.00      1.00      2727\n",
      "\n",
      "    accuracy                           1.00      7996\n",
      "   macro avg       1.00      1.00      1.00      7996\n",
      "weighted avg       1.00      1.00      1.00      7996\n",
      "\n",
      "==================================================\n",
      "Results for Deep Learning:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-score: 1.0000\n",
      "Confusion Matrix:\n",
      " [[2702    0    0]\n",
      " [   0 2567    0]\n",
      " [   0    0 2727]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2702\n",
      "           1       1.00      1.00      1.00      2567\n",
      "           2       1.00      1.00      1.00      2727\n",
      "\n",
      "    accuracy                           1.00      7996\n",
      "   macro avg       1.00      1.00      1.00      7996\n",
      "weighted avg       1.00      1.00      1.00      7996\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Prediction Function\n",
    "def predict_sentiment(review_text):\n",
    "    review_text_cleaned = cleaner.clean_text(review_text)\n",
    "    if best_model_name == \"Deep Learning\":\n",
    "        sequence = pad_sequences(tokenizer.texts_to_sequences([review_text_cleaned]), maxlen=100, padding=\"post\")\n",
    "        prediction = np.argmax(best_model.predict(sequence), axis=1)[0]\n",
    "    else:\n",
    "        prediction = best_model.predict(vectorizer.transform([review_text_cleaned]))[0]\n",
    "    sentiment_labels = {1: \"Positive\", 0: \"Negative\", 2: \"Neutral\"}\n",
    "    return sentiment_labels.get(prediction, \"Unknown\")\n",
    "\n",
    "# Obtain raw connection to use cursor functionality\n",
    "connection = engine.raw_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Execute a query to fetch flight reviews for inference\n",
    "cursor.execute(\"SELECT * FROM Reviews\")\n",
    "inference_reviews_data = cursor.fetchall()\n",
    "\n",
    "# Convert the fetched reviews into a pandas DataFrame\n",
    "columns = [desc[0] for desc in cursor.description]  # Get column names\n",
    "inference_reviews = pd.DataFrame(inference_reviews_data, columns=columns)\n",
    "\n",
    "# Apply the sentiment prediction function to the reviews\n",
    "inference_reviews[\"Predicted_Sentiment\"] = inference_reviews[\"review\"].apply(predict_sentiment)\n",
    "\n",
    "\n",
    "# Print the DataFrame with predicted sentiments\n",
    "print(inference_reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              reviews  predicted_sentiment\n",
      "0      The boarding process was smooth and efficient.                    1\n",
      "1         The in-flight WiFi was slow and unreliable.                    0\n",
      "2   The staff went above and beyond to make sure w...                    1\n",
      "3           The seats were cramped and uncomfortable.                    0\n",
      "4                  The flight was quiet and peaceful.                    0\n",
      "5            The food was tasteless and unappetizing.                    0\n",
      "6   The cabin crew were very polite and accommodat...                    0\n",
      "7    I was impressed by the cleanliness of the plane.                    0\n",
      "8   The flight attendants were rude and unprofessi...                    0\n",
      "9   The departure and arrival times were accurate ...                    0\n",
      "10  Exceptional service! The crew was attentive an...                    0\n",
      "11  The seats were uncomfortable and there was no ...                    0\n",
      "12          Check-in process was quick and efficient.                    0\n",
      "13         The plane was old and not well-maintained.                    0\n",
      "14       Flight attendants were friendly and helpful.                    0\n",
      "15  There were no power outlets to charge my devices.                    2\n",
      "16         Food was surprisingly good for an airline.                    2\n",
      "17                        The bathroom was not clean.                    2\n",
      "18  The boarding process was chaotic and disorgani...                    0\n",
      "19  I appreciated the complimentary snacks and bev...                    0\n",
      "20            The flight was bumpy due to turbulence.                    2\n",
      "21                  The cabin was clean and well-lit.                    0\n",
      "22              Long wait times at the baggage claim.                    2\n",
      "Predictions saved to sentiment_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Prediction Function\n",
    "def predict_sentiment(review_text):\n",
    "    review_text_cleaned = cleaner.clean_text(review_text)\n",
    "    if best_model_name == \"Deep Learning\":\n",
    "        sequence = pad_sequences(tokenizer.texts_to_sequences([review_text_cleaned]), maxlen=100, padding=\"post\")\n",
    "        prediction = np.argmax(best_model.predict(sequence), axis=1)[0]\n",
    "    else:\n",
    "        prediction = best_model.predict(vectorizer.transform([review_text_cleaned]))[0]\n",
    "    sentiment_labels = {1: \"Positive\", 0: \"Negative\", 2: \"Neutral\"}\n",
    "    return sentiment_labels.get(prediction, \"Unknown\")\n",
    "\n",
    "# Obtain raw connection to use cursor functionality\n",
    "connection = engine.raw_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Execute a query to fetch flight reviews for inference\n",
    "cursor.execute(\"SELECT * FROM Reviews\")\n",
    "inference_reviews_data = cursor.fetchall()\n",
    "\n",
    "# Convert the fetched reviews into a pandas DataFrame\n",
    "columns = [desc[0] for desc in cursor.description]  # Get column names\n",
    "inference_reviews = pd.DataFrame(inference_reviews_data, columns=columns)\n",
    "\n",
    "# Apply the sentiment prediction function to the reviews\n",
    "inference_reviews[\"Predicted_Sentiment\"] = inference_reviews[\"review\"].apply(predict_sentiment)\n",
    "\n",
    "\n",
    "# Print the DataFrame with predicted sentiments\n",
    "print(inference_reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Text Cleaning Transformer\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.apply(self.clean_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "# ✅ Model Selection Logic (Placeholder)\n",
    "def select_best_model(rf_acc, xgb_acc, dl_acc):\n",
    "    \"\"\"Select the best model based on accuracy.\"\"\"\n",
    "    if rf_acc >= xgb_acc and rf_acc >= dl_acc:\n",
    "        return \"Random Forest\"\n",
    "    elif xgb_acc >= dl_acc:\n",
    "        return \"XGBoost\"\n",
    "    else:\n",
    "        return \"Deep Learning\"\n",
    "\n",
    "# ✅ TF-IDF + ML Model Pipeline\n",
    "ml_pipeline = Pipeline([\n",
    "    ('text_cleaning', TextCleaner()),\n",
    "    ('vectorizer', TfidfVectorizer(max_features=50000)),\n",
    "    ('classifier', None)  # Placeholder for RF/XGBoost model\n",
    "])\n",
    "\n",
    "# ✅ Tokenization + Deep Learning Pipeline\n",
    "dl_pipeline = Pipeline([\n",
    "    ('text_cleaning', TextCleaner()),\n",
    "    ('tokenizer', None),  # Placeholder for tokenizer\n",
    "    ('pad_sequences', None),  # Placeholder for sequence padding\n",
    "    ('classifier', None)  # Placeholder for DL model\n",
    "])\n",
    "\n",
    "# ✅ Sentiment Prediction Logic\n",
    "def predict_sentiment(review_text, best_model_name):\n",
    "    \"\"\"Predict sentiment based on the selected best model.\"\"\"\n",
    "    review_text_cleaned = TextCleaner().clean_text(review_text)\n",
    "    \n",
    "    if best_model_name == \"Deep Learning\":\n",
    "        sequence = pad_sequences(None, maxlen=100, padding=\"post\")  # Placeholder for tokenizer\n",
    "        prediction = np.argmax(None.predict(sequence), axis=1)[0]  # Placeholder for DL model\n",
    "    else:\n",
    "        prediction = None.predict(None.transform([review_text_cleaned]))[0]  # Placeholder for ML model\n",
    "\n",
    "    sentiment_labels = {1: \"Positive\", 0: \"Negative\", 2: \"Neutral\"}\n",
    "    return sentiment_labels.get(prediction, \"Unknown\")\n",
    "\n",
    "\n",
    "# 1️⃣ Fetch Reviews from Database\n",
    "# 2️⃣ Preprocess Text (Cleaning)\n",
    "# 3️⃣ Train TF-IDF + ML Models (RF/XGBoost)\n",
    "# 4️⃣ Train Tokenizer + Deep Learning Model\n",
    "# 5️⃣ Evaluate Models & Select Best One\n",
    "# 6️⃣ Deploy Best Model in Sentiment Prediction Pipeline\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
